{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: Fill in a module description here\n",
    "output-file: environements_basic.html\n",
    "title: environments.basic\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp environments.basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class SimpleTradingEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    A basic Gymnasium environment for trading simulation.\n",
    "\n",
    "    This environment simulates trading a single cryptocurrency using a simplified action space\n",
    "    and observation space. It's designed to be a starting point for reinforcement learning\n",
    "    experiments in crypto trading and prioritizes clarity and ease of use.\n",
    "\n",
    "    Key Features:\n",
    "        - Discrete action space: Buy, Sell, Hold.\n",
    "        - Observation space: Current price, crypto held, cash balance.\n",
    "        - Simple reward function: Portfolio value change.\n",
    "        - Basic transaction cost simulation.\n",
    "        - Clear comments and adherence to best software practices.\n",
    "\n",
    "    Important Notes:\n",
    "        - This is a *simplified* environment. It does not include many real-world complexities\n",
    "          like order book dynamics, slippage (beyond basic transaction cost), market impact,\n",
    "          multiple cryptocurrencies, or external factors influencing price.\n",
    "        - The price data is pre-loaded and deterministic, making it suitable for initial\n",
    "          algorithm testing and development but not for realistic backtesting or live trading.\n",
    "        - For more advanced and realistic simulations, consider building upon this foundation\n",
    "          or using more sophisticated trading environment libraries.\n",
    "\n",
    "    Best Practices Implemented:\n",
    "        - **Clear Class and Method Docstrings:** Explaining the purpose and usage of each component.\n",
    "        - **Type Hinting:** Improving code readability and helping catch type-related errors.\n",
    "        - **Meaningful Variable Names:** Enhancing code understanding.\n",
    "        - **Modular Structure:**  Methods for `step`, `reset`, `render`, etc., as per Gym standards.\n",
    "        - **Configuration via `__init__`:**  Allowing customization of environment parameters.\n",
    "        - **Error Handling (Basic):**  Checking for invalid actions (e.g., selling without crypto).\n",
    "        - **Concise and Focused Code:**  Avoiding unnecessary complexity for a basic environment.\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {\n",
    "        \"render.modes\": [\"human\", \"rgb_array\"]\n",
    "    }  # Optional rendering modes (currently only 'human' for text output)\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        price_data: np.ndarray,\n",
    "        initial_cash: float = 10000,\n",
    "        transaction_cost_percent: float = 0.001,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the CryptoTradingEnv environment.\n",
    "\n",
    "        Args:\n",
    "            price_data (np.ndarray): 1D numpy array of historical cryptocurrency prices.\n",
    "                                      Each element represents the price at a time step.\n",
    "            initial_cash (float, optional): Starting cash balance. Defaults to 10000.\n",
    "            transaction_cost_percent (float, optional): Transaction cost as a percentage of trade value. Defaults to 0.001 (0.1%).\n",
    "        \"\"\"\n",
    "        super(\n",
    "            CryptoTradingEnv, self\n",
    "        ).__init__()  # Call superclass constructor to initialize gym.Env\n",
    "\n",
    "        # --- Input Validation and Data Storage ---\n",
    "        if not isinstance(price_data, np.ndarray) or price_data.ndim != 1:\n",
    "            raise ValueError(\"price_data must be a 1D numpy array.\")\n",
    "        if not np.issubdtype(price_data.dtype, np.number):\n",
    "            raise ValueError(\"price_data must contain numerical values (prices).\")\n",
    "        if initial_cash <= 0:\n",
    "            raise ValueError(\"initial_cash must be a positive value.\")\n",
    "        if (\n",
    "            not 0 <= transaction_cost_percent < 1\n",
    "        ):  # Transaction cost should be between 0 and 1 (exclusive of 1)\n",
    "            raise ValueError(\n",
    "                \"transaction_cost_percent must be between 0 and 1 (exclusive of 1).\"\n",
    "            )\n",
    "\n",
    "        self.price_data = price_data\n",
    "        self.initial_cash = initial_cash\n",
    "        self.transaction_cost_percent = transaction_cost_percent\n",
    "\n",
    "        # --- Define Action and Observation Space ---\n",
    "        # Action space: Discrete 3 actions: 0 = Hold, 1 = Buy, 2 = Sell\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "        # Observation space: Dictionary containing 'price', 'crypto_held', 'cash_balance'\n",
    "        # We use Box for continuous values (price, cash, crypto holding - although crypto_held could be discrete if we only trade in whole units, but for flexibility we keep it continuous)\n",
    "        self.observation_space = spaces.Dict(\n",
    "            {\n",
    "                \"price\": spaces.Box(\n",
    "                    low=0, high=np.inf, shape=(1,), dtype=np.float32\n",
    "                ),  # Current price (single value)\n",
    "                \"crypto_held\": spaces.Box(\n",
    "                    low=0, high=np.inf, shape=(1,), dtype=np.float32\n",
    "                ),  # Amount of cryptocurrency held\n",
    "                \"cash_balance\": spaces.Box(\n",
    "                    low=0, high=np.inf, shape=(1,), dtype=np.float32\n",
    "                ),  # Current cash balance\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # --- Initialize Environment State ---\n",
    "        self.current_step = 0  # Track the current time step in the price data\n",
    "        self.cash_balance = initial_cash\n",
    "        self.crypto_held = 0.0  # Start with no cryptocurrency\n",
    "        self.portfolio_value = (\n",
    "            initial_cash  # Initial portfolio value is just the initial cash\n",
    "        )\n",
    "        self.net_worth_history = [\n",
    "            initial_cash\n",
    "        ]  # Keep track of net worth over time for analysis/rendering\n",
    "\n",
    "        # --- Placeholder for rendering (can be expanded for more sophisticated visualization) ---\n",
    "        self.renderer = None  # Placeholder for a renderer object if you want to add visual rendering\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        \"\"\"\n",
    "        Resets the environment to its initial state.\n",
    "\n",
    "        This is called at the beginning of each episode.\n",
    "\n",
    "        Args:\n",
    "            seed (int, optional): Seed for random number generator (not used in this deterministic env but included for Gym API compliance). Defaults to None.\n",
    "            options (dict, optional): Options for reset (not used in this basic env but included for Gym API compliance). Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Initial observation and info dictionary.\n",
    "                   - observation (dict): Initial observation dictionary as defined by self.observation_space.\n",
    "                   - info (dict):  Additional information (can be empty in this basic env).\n",
    "        \"\"\"\n",
    "        super().reset(\n",
    "            seed=seed, options=options\n",
    "        )  # Call superclass reset for seed handling (even though not used here)\n",
    "\n",
    "        self.current_step = 0\n",
    "        self.cash_balance = self.initial_cash\n",
    "        self.crypto_held = 0.0\n",
    "        self.portfolio_value = self.initial_cash\n",
    "        self.net_worth_history = [self.initial_cash]\n",
    "\n",
    "        observation = self._get_observation()  # Get initial observation\n",
    "        info = {}  # Empty info dictionary for now\n",
    "\n",
    "        return observation, info\n",
    "\n",
    "    def _get_observation(self):\n",
    "        \"\"\"\n",
    "        Helper function to construct the observation dictionary for the current state.\n",
    "\n",
    "        Returns:\n",
    "            dict: Observation dictionary containing 'price', 'crypto_held', and 'cash_balance'.\n",
    "        \"\"\"\n",
    "        current_price = self.price_data[self.current_step]\n",
    "        return {\n",
    "            \"price\": np.array(\n",
    "                [current_price], dtype=np.float32\n",
    "            ),  # Ensure price is a numpy array of shape (1,) and float32\n",
    "            \"crypto_held\": np.array(\n",
    "                [self.crypto_held], dtype=np.float32\n",
    "            ),  # Ensure crypto_held is a numpy array of shape (1,) and float32\n",
    "            \"cash_balance\": np.array(\n",
    "                [self.cash_balance], dtype=np.float32\n",
    "            ),  # Ensure cash_balance is a numpy array of shape (1,) and float32\n",
    "        }\n",
    "\n",
    "    def step(self, action: int):\n",
    "        \"\"\"\n",
    "        Executes one time step within the environment.\n",
    "\n",
    "        This function takes an action, updates the environment state based on the action and price data,\n",
    "        and returns the next observation, reward, termination status, truncation status, and info dictionary.\n",
    "\n",
    "        Args:\n",
    "            action (int): Action to take. Must be one of:\n",
    "                          0 = Hold, 1 = Buy, 2 = Sell.\n",
    "\n",
    "        Returns:\n",
    "            tuple: observation, reward, terminated, truncated, info\n",
    "                   - observation (dict): Next observation dictionary.\n",
    "                   - reward (float): Reward obtained in this step.\n",
    "                   - terminated (bool): True if the episode is terminated (e.g., end of price data).\n",
    "                   - truncated (bool): True if the episode is truncated (not used in this basic env, always False).\n",
    "                   - info (dict):  Additional information (can be empty in this basic env).\n",
    "        \"\"\"\n",
    "        # --- Input Validation ---\n",
    "        if action not in [0, 1, 2]:\n",
    "            raise ValueError(\n",
    "                f\"Invalid action: {action}. Action must be in [0, 1, 2] (Hold, Buy, Sell).\"\n",
    "            )\n",
    "\n",
    "        current_price = self.price_data[self.current_step]\n",
    "        prev_portfolio_value = (\n",
    "            self.portfolio_value\n",
    "        )  # Store previous portfolio value to calculate reward\n",
    "\n",
    "        # --- Execute Action ---\n",
    "        transaction_cost = 0  # Initialize transaction cost for this step\n",
    "\n",
    "        if action == 1:  # Buy\n",
    "            # Buy with all available cash (for simplicity in this basic env)\n",
    "            amount_to_buy = self.cash_balance\n",
    "            if amount_to_buy > 0:  # Ensure we have cash to buy\n",
    "                transaction_cost = amount_to_buy * self.transaction_cost_percent\n",
    "                crypto_to_buy = (amount_to_buy - transaction_cost) / current_price\n",
    "                self.crypto_held += crypto_to_buy\n",
    "                self.cash_balance -= amount_to_buy\n",
    "        elif action == 2:  # Sell\n",
    "            # Sell all crypto held (for simplicity in this basic env)\n",
    "            amount_to_sell = self.crypto_held * current_price\n",
    "            if amount_to_sell > 0:  # Ensure we have crypto to sell\n",
    "                transaction_cost = amount_to_sell * self.transaction_cost_percent\n",
    "                cash_from_sale = amount_to_sell - transaction_cost\n",
    "                self.cash_balance += cash_from_sale\n",
    "                self.crypto_held = 0.0\n",
    "        elif action == 0:  # Hold\n",
    "            pass  # Do nothing, just hold current positions\n",
    "\n",
    "        # --- Update Environment State ---\n",
    "        self.current_step += 1\n",
    "\n",
    "        # Calculate portfolio value at the end of the step (cash + value of crypto holdings)\n",
    "        self.portfolio_value = self.cash_balance + self.crypto_held * current_price\n",
    "        self.net_worth_history.append(self.portfolio_value)\n",
    "\n",
    "        # --- Calculate Reward ---\n",
    "        # Simple reward: change in portfolio value\n",
    "        reward = self.portfolio_value - prev_portfolio_value\n",
    "\n",
    "        # --- Check for Termination ---\n",
    "        terminated = (\n",
    "            self.current_step >= len(self.price_data) - 1\n",
    "        )  # End episode when we reach the end of price data\n",
    "        truncated = False  # Truncation is not used in this basic environment\n",
    "\n",
    "        # --- Get Next Observation and Info ---\n",
    "        observation = self._get_observation()\n",
    "        info = {\n",
    "            \"step\": self.current_step,\n",
    "            \"price\": current_price,\n",
    "            \"transaction_cost\": transaction_cost,\n",
    "            \"action\": action,  # Useful for debugging and analysis\n",
    "            \"portfolio_value\": self.portfolio_value,\n",
    "            \"cash_balance\": self.cash_balance,\n",
    "            \"crypto_held\": self.crypto_held,\n",
    "            \"reward\": reward,\n",
    "            # Add any other relevant information you want to track in 'info'\n",
    "        }\n",
    "\n",
    "        return observation, reward, terminated, truncated, info\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        \"\"\"\n",
    "        Renders the environment.\n",
    "\n",
    "        In 'human' mode, it prints the current state to the console.\n",
    "        For 'rgb_array' mode (not fully implemented here), it should return an RGB array representing the environment.\n",
    "\n",
    "        Args:\n",
    "            mode (str, optional): Rendering mode. 'human' (text output) or 'rgb_array' (not fully implemented). Defaults to 'human'.\n",
    "        \"\"\"\n",
    "        if mode == \"human\":\n",
    "            print(f\"------------------ Step: {self.current_step} ------------------\")\n",
    "            print(f\"Price: ${self.price_data[self.current_step]:.2f}\")\n",
    "            print(\n",
    "                f\"Action: {self._get_action_name(self.step_history[-1]['action'] if hasattr(self, 'step_history') and self.step_history else 'Initial')}\"\n",
    "            )  # Access last action from info if available\n",
    "            print(f\"Crypto Held: {self.crypto_held:.4f}\")\n",
    "            print(f\"Cash Balance: ${self.cash_balance:.2f}\")\n",
    "            print(f\"Portfolio Value: ${self.portfolio_value:.2f}\")\n",
    "            print(\n",
    "                f\"Reward: {self.step_history[-1]['reward']:.4f}\"\n",
    "                if hasattr(self, \"step_history\") and self.step_history\n",
    "                else \"Initial\"\n",
    "            )  # Access last reward from info if available\n",
    "            print(\"----------------------------------------------------\")\n",
    "        elif mode == \"rgb_array\":\n",
    "            # For more advanced rendering (e.g., using matplotlib to plot portfolio value), you would\n",
    "            # generate an image as a numpy array here and return it.\n",
    "            # This is a placeholder for future expansion.\n",
    "            super().render_modes = [\n",
    "                \"rgb_array\",\n",
    "                \"human\",\n",
    "            ]  # Define supported render modes\n",
    "            return (\n",
    "                self._render_frame()\n",
    "            )  # You would need to implement _render_frame to return an RGB array\n",
    "        else:\n",
    "            super().render_modes = [\n",
    "                \"rgb_array\",\n",
    "                \"human\",\n",
    "            ]  # Define supported render modes\n",
    "            super().render()  # Call default render method if mode is not recognized\n",
    "\n",
    "    def _get_action_name(self, action_code):\n",
    "        \"\"\"Helper function to get action name from action code for rendering.\"\"\"\n",
    "        if action_code == 0:\n",
    "            return \"Hold\"\n",
    "        elif action_code == 1:\n",
    "            return \"Buy\"\n",
    "        elif action_code == 2:\n",
    "            return \"Sell\"\n",
    "        else:\n",
    "            return \"Unknown\"\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        Closes the environment and performs any necessary cleanup.\n",
    "\n",
    "        In this basic environment, there's no specific cleanup needed, but this method is included for Gym API compliance\n",
    "        and for potential future expansion where resources might need to be released.\n",
    "        \"\"\"\n",
    "        if self.renderer:\n",
    "            self.renderer.close()  # If you add a renderer, close it here\n",
    "        super().close()  # Call superclass close method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Tuple, Optional, Union\n",
    "\n",
    "\n",
    "class CryptoTradingEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    A cryptocurrency trading environment for reinforcement learning.\n",
    "\n",
    "    This environment simulates trading a single cryptocurrency based on historical price data.\n",
    "    It supports buying, selling, and holding actions, and provides observations\n",
    "    that can include price history, technical indicators, and the agent's current holdings.\n",
    "\n",
    "    Key Concepts:\n",
    "        -   Action Space:  Discrete (buy, sell, hold).  Could be extended to continuous.\n",
    "        -   Observation Space: Box (multidimensional array).  Shape depends on features.\n",
    "        -   Reward Function:  Change in portfolio value (can be customized).\n",
    "        -   Episode Termination:  End of data or when the agent runs out of funds.\n",
    "        -   Data: Requires a pandas DataFrame with 'close' prices (and optionally other features).\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {\n",
    "        \"render_modes\": [\"human\"],\n",
    "        \"render_fps\": 30,\n",
    "    }  # For future rendering capabilities\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: pd.DataFrame,\n",
    "        initial_balance: float = 10000.0,\n",
    "        transaction_fee_percent: float = 0.001,  # 0.1% transaction fee\n",
    "        lookback_window_size: int = 50,  # Number of past timesteps to include in observation\n",
    "        features: list[str] = [\"close\"],  # List of columns to use as features\n",
    "        render_mode: Optional[str] = None,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the CryptoTradingEnv.\n",
    "\n",
    "        Args:\n",
    "            data (pd.DataFrame):  Historical price data (and other features).  MUST include a 'close' column.\n",
    "            initial_balance (float): Starting capital for the agent.\n",
    "            transaction_fee_percent (float):  Fee percentage per transaction (e.g., 0.001 for 0.1%).\n",
    "            lookback_window_size (int):  How many past timesteps to use in the observation.\n",
    "            features (List[str]):  Column names from 'data' to use as observation features.\n",
    "            render_mode (str, optional): Render mode for human visualization. Defaults to None.\n",
    "        Raises:\n",
    "            ValueError: If input data is invalid or required columns are missing.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        if not isinstance(data, pd.DataFrame):\n",
    "            raise ValueError(\"Data must be a pandas DataFrame.\")\n",
    "        if \"close\" not in data.columns:\n",
    "            raise ValueError(\"Data must contain a 'close' column for price data.\")\n",
    "        if not all(feature in data.columns for feature in features):\n",
    "            raise ValueError(\"All features must be present in the data columns.\")\n",
    "        if lookback_window_size <= 0:\n",
    "            raise ValueError(\"lookback_window_size must be greater than 0.\")\n",
    "\n",
    "        self.data = data\n",
    "        self.features = features\n",
    "        self.initial_balance = initial_balance\n",
    "        self.transaction_fee_percent = transaction_fee_percent\n",
    "        self.lookback_window_size = lookback_window_size\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        # Action Space: 0 = Hold, 1 = Buy, 2 = Sell\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "\n",
    "        # Observation Space: (lookback_window_size, num_features) + balance + holdings\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf,\n",
    "            high=np.inf,\n",
    "            shape=(lookback_window_size, len(self.features) + 2),\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "\n",
    "        self.current_step = lookback_window_size\n",
    "        self.balance = initial_balance\n",
    "        self.holdings = 0.0  # Amount of crypto held\n",
    "        self.portfolio_value = initial_balance  # balance + (holdings * current_price)\n",
    "        self._done = False  # added to track done state explicitly\n",
    "\n",
    "    def _get_observation(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Constructs the observation array.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray:  The observation array.\n",
    "        \"\"\"\n",
    "        window_data = self.data[self.features][\n",
    "            self.current_step - self.lookback_window_size : self.current_step\n",
    "        ].values\n",
    "\n",
    "        # Add balance and holdings to the observation.  Reshape for consistent shape.\n",
    "        balance_and_holdings = np.array([[self.balance, self.holdings]])\n",
    "        repeated_balance_holdings = np.repeat(\n",
    "            balance_and_holdings, self.lookback_window_size, axis=0\n",
    "        )\n",
    "        observation = np.concatenate((window_data, repeated_balance_holdings), axis=1)\n",
    "\n",
    "        return observation.astype(np.float32)\n",
    "\n",
    "    def _calculate_reward(self, action: int) -> float:\n",
    "        \"\"\"\n",
    "        Calculates the reward based on the change in portfolio value.\n",
    "\n",
    "        Args:\n",
    "            action (int): The action taken by the agent.\n",
    "\n",
    "        Returns:\n",
    "            float: The reward for the current step.  (Can be customized).\n",
    "        \"\"\"\n",
    "        current_price = self.data[\"close\"][self.current_step]\n",
    "        previous_portfolio_value = self.portfolio_value\n",
    "        self.portfolio_value = self.balance + (self.holdings * current_price)\n",
    "        reward = self.portfolio_value - previous_portfolio_value\n",
    "        return reward\n",
    "\n",
    "    def _execute_trade(self, action: int) -> None:\n",
    "        \"\"\"\n",
    "        Executes a trade (buy or sell) and updates balance/holdings.\n",
    "\n",
    "        Args:\n",
    "            action (int): The action taken (1 for buy, 2 for sell).\n",
    "        \"\"\"\n",
    "\n",
    "        current_price = self.data[\"close\"][self.current_step]\n",
    "\n",
    "        if action == 1:  # Buy\n",
    "            affordable_amount = self.balance / current_price\n",
    "            cost = affordable_amount * current_price\n",
    "            fee = cost * self.transaction_fee_percent\n",
    "            if self.balance >= cost + fee:\n",
    "                self.balance -= cost + fee\n",
    "                self.holdings += affordable_amount * (1 - self.transaction_fee_percent)\n",
    "\n",
    "        elif action == 2:  # Sell\n",
    "            fee = self.holdings * current_price * self.transaction_fee_percent\n",
    "            self.balance += self.holdings * current_price - fee\n",
    "            self.holdings = 0.0\n",
    "\n",
    "    def step(self, action: int) -> Tuple[np.ndarray, float, bool, bool, dict]:\n",
    "        \"\"\"\n",
    "        Takes a step in the environment.\n",
    "\n",
    "        Args:\n",
    "            action (int): The action to take (0, 1, or 2).\n",
    "\n",
    "        Returns:\n",
    "            Tuple: A tuple containing:\n",
    "                - observation (np.ndarray): The next state's observation.\n",
    "                - reward (float): The reward for the action.\n",
    "                - terminated (bool): Whether the episode has ended.\n",
    "                - truncated (bool): Whether the episode was truncated (not used here, but required by gym API).\n",
    "                - info (dict): Additional information (empty in this case).\n",
    "        \"\"\"\n",
    "        if self._done:\n",
    "            # If the episode is already done, you can either reset it or raise an error.\n",
    "            # Here we reset.  Alternatively, you can raise:  raise Exception(\"Episode already done\")\n",
    "            return self.reset()[0], 0.0, True, False, {}\n",
    "\n",
    "        self._execute_trade(action)  # Execute the trade first\n",
    "        reward = self._calculate_reward(action)  # Calculate reward *after* trade\n",
    "\n",
    "        self.current_step += 1\n",
    "        # Check if the episode is done (either end of data or out of money)\n",
    "        self._done = self.current_step >= len(self.data) - 1 or self.balance <= 0\n",
    "        truncated = False\n",
    "        observation = self._get_observation()\n",
    "        info: dict = {}\n",
    "        return observation, reward, self._done, truncated, info  # type: ignore\n",
    "\n",
    "    def reset(\n",
    "        self, *, seed: Optional[int] = None, options: Optional[dict] = None\n",
    "    ) -> Tuple[np.ndarray, dict]:\n",
    "        \"\"\"\n",
    "        Resets the environment to the initial state.\n",
    "\n",
    "        Args:\n",
    "            seed (Optional[int]): Seed for random number generation.\n",
    "            options (Optional[dict]):  Additional options (not used here).\n",
    "\n",
    "        Returns:\n",
    "            Tuple: A tuple containing:\n",
    "                - observation (np.ndarray): The initial observation.\n",
    "                - info (dict):  Additional info (empty in this case).\n",
    "        \"\"\"\n",
    "        super().reset(seed=seed)  # Call gym.Env's reset with the seed\n",
    "\n",
    "        self.current_step = self.lookback_window_size\n",
    "        self.balance = self.initial_balance\n",
    "        self.holdings = 0.0\n",
    "        self.portfolio_value = self.initial_balance\n",
    "        self._done = False  # Reset the done flag\n",
    "        observation = self._get_observation()\n",
    "        info: dict = {}\n",
    "        return observation, info\n",
    "\n",
    "    def render(self) -> None:\n",
    "        \"\"\"\n",
    "        Renders the environment (not implemented here, but could be used for visualization).\n",
    "        \"\"\"\n",
    "        # Future implementation for visualization (e.g., using matplotlib).\n",
    "        if self.render_mode == \"human\":\n",
    "            print(\n",
    "                f\"Step: {self.current_step}, Balance: {self.balance:.2f}, \"\n",
    "                f\"Holdings: {self.holdings:.4f}, Portfolio Value: {self.portfolio_value:.2f}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
