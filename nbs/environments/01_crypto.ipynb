{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: Fill in a module description here\n",
    "output-file: environments_crypto.html\n",
    "title: environments.crypto\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp environements.crypto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import gymnasium as gym  # Import the gymnasium library for creating the environment\n",
    "from gymnasium import spaces  # Import spaces for defining action and observation spaces\n",
    "import numpy as np  # Import numpy for numerical operations\n",
    "import pandas as pd  # Import pandas for handling data\n",
    "from typing import Tuple, Optional, Union  # Import typing for type annotations\n",
    "\n",
    "class CryptoTradingEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    A cryptocurrency trading environment for reinforcement learning.\n",
    "\n",
    "    This environment simulates trading a single cryptocurrency based on historical price data.\n",
    "    It supports buying, selling, and holding actions, and provides observations \n",
    "    that can include price history, technical indicators, and the agent's current holdings.\n",
    "\n",
    "    Key Concepts:\n",
    "        -   Action Space:  Discrete (buy, sell, hold).  Could be extended to continuous.\n",
    "        -   Observation Space: Box (multidimensional array).  Shape depends on features.\n",
    "        -   Reward Function:  Change in portfolio value (can be customized).\n",
    "        -   Episode Termination:  End of data or when the agent runs out of funds.\n",
    "        -   Data: Requires a pandas DataFrame with 'close' prices (and optionally other features).\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 30}  # For future rendering capabilities\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: pd.DataFrame,\n",
    "        initial_balance: float = 10000.0,\n",
    "        transaction_fee_percent: float = 0.001,  # 0.1% transaction fee\n",
    "        lookback_window_size: int = 50,  # Number of past timesteps to include in observation\n",
    "        features: list[str] = [\"close\"],  # List of columns to use as features\n",
    "        render_mode: Optional[str] = None,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the CryptoTradingEnv.\n",
    "\n",
    "        Args:\n",
    "            data (pd.DataFrame):  Historical price data (and other features).  MUST include a 'close' column.\n",
    "            initial_balance (float): Starting capital for the agent.\n",
    "            transaction_fee_percent (float):  Fee percentage per transaction (e.g., 0.001 for 0.1%).\n",
    "            lookback_window_size (int):  How many past timesteps to use in the observation.\n",
    "            features (List[str]):  Column names from 'data' to use as observation features.\n",
    "            render_mode (str, optional): Render mode for human visualization. Defaults to None.\n",
    "        Raises:\n",
    "            ValueError: If input data is invalid or required columns are missing.\n",
    "        \"\"\"\n",
    "        super().__init__()  # Initialize the base gym environment\n",
    "\n",
    "        if not isinstance(data, pd.DataFrame):\n",
    "            raise ValueError(\"Data must be a pandas DataFrame.\")  # Ensure data is a DataFrame\n",
    "        if \"close\" not in data.columns:\n",
    "            raise ValueError(\"Data must contain a 'close' column for price data.\")  # Ensure 'close' column exists\n",
    "        if not all(feature in data.columns for feature in features):\n",
    "            raise ValueError(\"All features must be present in the data columns.\")  # Ensure all features exist\n",
    "        if lookback_window_size <= 0:\n",
    "            raise ValueError(\"lookback_window_size must be greater than 0.\")  # Ensure lookback window size is positive\n",
    "\n",
    "        self.data = data  # Store the data\n",
    "        self.features = features  # Store the features\n",
    "        self.initial_balance = initial_balance  # Store the initial balance\n",
    "        self.transaction_fee_percent = transaction_fee_percent  # Store the transaction fee percentage\n",
    "        self.lookback_window_size = lookback_window_size  # Store the lookback window size\n",
    "        self.render_mode = render_mode  # Store the render mode\n",
    "\n",
    "        # Action Space: 0 = Hold, 1 = Buy, 2 = Sell\n",
    "        self.action_space = spaces.Discrete(3)  # Define the action space\n",
    "\n",
    "        # Observation Space: (lookback_window_size, num_features) + balance + holdings\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf,\n",
    "            high=np.inf,\n",
    "            shape=(lookback_window_size, len(self.features) + 2),\n",
    "            dtype=np.float32,\n",
    "        )  # Define the observation space\n",
    "\n",
    "        self.current_step = lookback_window_size  # Initialize the current step\n",
    "        self.balance = initial_balance  # Initialize the balance\n",
    "        self.holdings = 0.0  # Initialize the holdings\n",
    "        self.portfolio_value = initial_balance  # Initialize the portfolio value\n",
    "        self._done = False  # Initialize the done flag\n",
    "\n",
    "    def _get_observation(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Constructs the observation array.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray:  The observation array.\n",
    "        \"\"\"\n",
    "        window_data = self.data[self.features][\n",
    "            self.current_step - self.lookback_window_size : self.current_step\n",
    "        ].values  # Get the window of data for the observation\n",
    "\n",
    "        # Add balance and holdings to the observation.  Reshape for consistent shape.\n",
    "        balance_and_holdings = np.array([[self.balance, self.holdings]])  # Create array for balance and holdings\n",
    "        repeated_balance_holdings = np.repeat(\n",
    "            balance_and_holdings, self.lookback_window_size, axis=0\n",
    "        )  # Repeat balance and holdings for each timestep in the window\n",
    "        observation = np.concatenate(\n",
    "            (window_data, repeated_balance_holdings), axis=1\n",
    "        )  # Concatenate window data with balance and holdings\n",
    "\n",
    "        return observation.astype(np.float32)  # Return the observation as float32\n",
    "\n",
    "    def _calculate_reward(self, action: int) -> float:\n",
    "        \"\"\"\n",
    "        Calculates the reward based on the change in portfolio value.\n",
    "\n",
    "        Args:\n",
    "            action (int): The action taken by the agent.\n",
    "\n",
    "        Returns:\n",
    "            float: The reward for the current step.  (Can be customized).\n",
    "        \"\"\"\n",
    "        current_price = self.data[\"close\"][self.current_step]  # Get the current price\n",
    "        previous_portfolio_value = self.portfolio_value  # Store the previous portfolio value\n",
    "        self.portfolio_value = self.balance + (self.holdings * current_price)  # Calculate the new portfolio value\n",
    "        reward = self.portfolio_value - previous_portfolio_value  # Calculate the reward as the change in portfolio value\n",
    "        return reward  # Return the reward\n",
    "\n",
    "    def _execute_trade(self, action: int) -> None:\n",
    "        \"\"\"\n",
    "        Executes a trade (buy or sell) and updates balance/holdings.\n",
    "\n",
    "        Args:\n",
    "            action (int): The action taken (1 for buy, 2 for sell).\n",
    "        \"\"\"\n",
    "\n",
    "        current_price = self.data[\"close\"][self.current_step]  # Get the current price\n",
    "\n",
    "        if action == 1:  # Buy\n",
    "            affordable_amount = self.balance / current_price  # Calculate the amount of crypto that can be bought\n",
    "            cost = affordable_amount * current_price  # Calculate the cost of the purchase\n",
    "            fee = cost * self.transaction_fee_percent  # Calculate the transaction fee\n",
    "            if self.balance >= cost + fee:  # Check if there is enough balance to cover the cost and fee\n",
    "                self.balance -= cost + fee  # Deduct the cost and fee from the balance\n",
    "                self.holdings += affordable_amount * (1-self.transaction_fee_percent)  # Update the holdings\n",
    "\n",
    "        elif action == 2:  # Sell\n",
    "            fee = self.holdings * current_price * self.transaction_fee_percent  # Calculate the transaction fee\n",
    "            self.balance += self.holdings * current_price - fee  # Add the proceeds from the sale to the balance\n",
    "            self.holdings = 0.0  # Reset the holdings to zero\n",
    "\n",
    "    def step(\n",
    "        self, action: int\n",
    "    ) -> Tuple[np.ndarray, float, bool, bool, dict]:\n",
    "        \"\"\"\n",
    "        Takes a step in the environment.\n",
    "\n",
    "        Args:\n",
    "            action (int): The action to take (0, 1, or 2).\n",
    "\n",
    "        Returns:\n",
    "            Tuple: A tuple containing:\n",
    "                - observation (np.ndarray): The next state's observation.\n",
    "                - reward (float): The reward for the action.\n",
    "                - terminated (bool): Whether the episode has ended.\n",
    "                - truncated (bool): Whether the episode was truncated (not used here, but required by gym API).\n",
    "                - info (dict): Additional information (empty in this case).\n",
    "        \"\"\"\n",
    "        if self._done:\n",
    "            # If the episode is already done, you can either reset it or raise an error.\n",
    "            # Here we reset.  Alternatively, you can raise:  raise Exception(\"Episode already done\")\n",
    "            return self.reset()[0], 0.0, True, False, {}  # Reset the environment if the episode is done\n",
    "        \n",
    "        self._execute_trade(action)  # Execute the trade first\n",
    "        reward = self._calculate_reward(action)  # Calculate reward *after* trade\n",
    "\n",
    "        self.current_step += 1  # Move to the next step\n",
    "        # Check if the episode is done (either end of data or out of money)\n",
    "        self._done = (\n",
    "            self.current_step >= len(self.data) - 1\n",
    "            or self.balance <= 0\n",
    "        )  # Set the done flag if the episode is finished\n",
    "        truncated = False  # Truncated is not used here\n",
    "        observation = self._get_observation()  # Get the next observation\n",
    "        info: dict = {}  # Additional info (empty in this case)\n",
    "        return observation, reward, self._done, truncated, info  # Return the step results\n",
    "\n",
    "    def reset(\n",
    "        self, *, seed: Optional[int] = None, options: Optional[dict] = None\n",
    "    ) -> Tuple[np.ndarray, dict]:\n",
    "        \"\"\"\n",
    "        Resets the environment to the initial state.\n",
    "\n",
    "        Args:\n",
    "            seed (Optional[int]): Seed for random number generation.\n",
    "            options (Optional[dict]):  Additional options (not used here).\n",
    "\n",
    "        Returns:\n",
    "            Tuple: A tuple containing:\n",
    "                - observation (np.ndarray): The initial observation.\n",
    "                - info (dict):  Additional info (empty in this case).\n",
    "        \"\"\"\n",
    "        super().reset(seed=seed)  # Call gym.Env's reset with the seed\n",
    "\n",
    "        self.current_step = self.lookback_window_size  # Reset the current step\n",
    "        self.balance = self.initial_balance  # Reset the balance\n",
    "        self.holdings = 0.0  # Reset the holdings\n",
    "        self.portfolio_value = self.initial_balance  # Reset the portfolio value\n",
    "        self._done = False  # Reset the done flag\n",
    "        observation = self._get_observation()  # Get the initial observation\n",
    "        info: dict = {}  # Additional info (empty in this case)\n",
    "        return observation, info  # Return the reset results\n",
    "\n",
    "    def render(self) -> None:\n",
    "        \"\"\"\n",
    "        Renders the environment (not implemented here, but could be used for visualization).\n",
    "        \"\"\"\n",
    "        # Future implementation for visualization (e.g., using matplotlib).\n",
    "        if self.render_mode == \"human\":\n",
    "            print(\n",
    "                f\"Step: {self.current_step}, Balance: {self.balance:.2f}, \"\n",
    "                f\"Holdings: {self.holdings:.4f}, Portfolio Value: {self.portfolio_value:.2f}\"\n",
    "            )  # Print the current state for human rendering\n",
    "\n",
    "# Example Usage (and testing)\n",
    "if __name__ == \"__main__\":\n",
    "    # Create sample data (replace with your actual data)\n",
    "    num_days = 200\n",
    "    data = pd.DataFrame(\n",
    "        {\n",
    "            \"close\": np.random.rand(num_days) * 100 + 100,  # Example prices\n",
    "            \"volume\": np.random.rand(num_days) * 1000 + 500,  # Example volume\n",
    "        }\n",
    "    )\n",
    "    data[\"returns\"] = data[\"close\"].pct_change()  # Calculate returns\n",
    "\n",
    "    # Create and use the environment\n",
    "    env = CryptoTradingEnv(\n",
    "        data=data,\n",
    "        initial_balance=1000,\n",
    "        transaction_fee_percent=0.001,\n",
    "        lookback_window_size=20,\n",
    "        features=[\"close\", \"volume\", \"returns\"],\n",
    "    )\n",
    "\n",
    "    # Basic interaction loop\n",
    "    obs, _ = env.reset()  # Reset the environment\n",
    "    n_steps = 50\n",
    "    for _ in range(n_steps):\n",
    "        action = env.action_space.sample()  # Replace with your agent's action\n",
    "        obs, reward, terminated, truncated, info = env.step(action)  # Take a step\n",
    "        done = terminated or truncated  # Check if the episode is done\n",
    "        env.render()  # Render the environment\n",
    "        if done:\n",
    "            print(\"Episode finished\")\n",
    "            obs, _ = env.reset()  # Reset if the episode is finished\n",
    "    print(\"Interaction loop finished.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
