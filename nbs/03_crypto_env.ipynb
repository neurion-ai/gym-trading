{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: Fill in a module description here\n",
    "output-file: core.html\n",
    "title: core\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Tuple, Optional, Union\n",
    "\n",
    "\n",
    "class CryptoTradingEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    A cryptocurrency trading environment for reinforcement learning.\n",
    "\n",
    "    This environment simulates trading a single cryptocurrency based on historical price data.\n",
    "    It supports buying, selling, and holding actions, and provides observations \n",
    "    that can include price history, technical indicators, and the agent's current holdings.\n",
    "\n",
    "    Key Concepts:\n",
    "        -   Action Space:  Discrete (buy, sell, hold).  Could be extended to continuous.\n",
    "        -   Observation Space: Box (multidimensional array).  Shape depends on features.\n",
    "        -   Reward Function:  Change in portfolio value (can be customized).\n",
    "        -   Episode Termination:  End of data or when the agent runs out of funds.\n",
    "        -   Data: Requires a pandas DataFrame with 'close' prices (and optionally other features).\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 30}  # For future rendering capabilities\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: pd.DataFrame,\n",
    "        initial_balance: float = 10000.0,\n",
    "        transaction_fee_percent: float = 0.001,  # 0.1% transaction fee\n",
    "        lookback_window_size: int = 50,  # Number of past timesteps to include in observation\n",
    "        features: list[str] = [\"close\"],  # List of columns to use as features\n",
    "        render_mode: Optional[str] = None,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the CryptoTradingEnv.\n",
    "\n",
    "        Args:\n",
    "            data (pd.DataFrame):  Historical price data (and other features).  MUST include a 'close' column.\n",
    "            initial_balance (float): Starting capital for the agent.\n",
    "            transaction_fee_percent (float):  Fee percentage per transaction (e.g., 0.001 for 0.1%).\n",
    "            lookback_window_size (int):  How many past timesteps to use in the observation.\n",
    "            features (List[str]):  Column names from 'data' to use as observation features.\n",
    "            render_mode (str, optional): Render mode for human visualization. Defaults to None.\n",
    "        Raises:\n",
    "            ValueError: If input data is invalid or required columns are missing.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        if not isinstance(data, pd.DataFrame):\n",
    "            raise ValueError(\"Data must be a pandas DataFrame.\")\n",
    "        if \"close\" not in data.columns:\n",
    "            raise ValueError(\"Data must contain a 'close' column for price data.\")\n",
    "        if not all(feature in data.columns for feature in features):\n",
    "            raise ValueError(\"All features must be present in the data columns.\")\n",
    "        if lookback_window_size <= 0:\n",
    "            raise ValueError(\"lookback_window_size must be greater than 0.\")\n",
    "\n",
    "        self.data = data\n",
    "        self.features = features\n",
    "        self.initial_balance = initial_balance\n",
    "        self.transaction_fee_percent = transaction_fee_percent\n",
    "        self.lookback_window_size = lookback_window_size\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        # Action Space: 0 = Hold, 1 = Buy, 2 = Sell\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "\n",
    "        # Observation Space: (lookback_window_size, num_features) + balance + holdings\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf,\n",
    "            high=np.inf,\n",
    "            shape=(lookback_window_size, len(self.features) + 2),\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "\n",
    "        self.current_step = lookback_window_size\n",
    "        self.balance = initial_balance\n",
    "        self.holdings = 0.0  # Amount of crypto held\n",
    "        self.portfolio_value = initial_balance  # balance + (holdings * current_price)\n",
    "        self._done = False  # added to track done state explicitly\n",
    "\n",
    "    def _get_observation(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Constructs the observation array.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray:  The observation array.\n",
    "        \"\"\"\n",
    "        window_data = self.data[self.features][\n",
    "            self.current_step - self.lookback_window_size : self.current_step\n",
    "        ].values\n",
    "\n",
    "        # Add balance and holdings to the observation.  Reshape for consistent shape.\n",
    "        balance_and_holdings = np.array([[self.balance, self.holdings]])\n",
    "        repeated_balance_holdings = np.repeat(\n",
    "            balance_and_holdings, self.lookback_window_size, axis=0\n",
    "        )\n",
    "        observation = np.concatenate(\n",
    "            (window_data, repeated_balance_holdings), axis=1\n",
    "        )\n",
    "\n",
    "        return observation.astype(np.float32)\n",
    "\n",
    "    def _calculate_reward(self, action: int) -> float:\n",
    "        \"\"\"\n",
    "        Calculates the reward based on the change in portfolio value.\n",
    "\n",
    "        Args:\n",
    "            action (int): The action taken by the agent.\n",
    "\n",
    "        Returns:\n",
    "            float: The reward for the current step.  (Can be customized).\n",
    "        \"\"\"\n",
    "        current_price = self.data[\"close\"][self.current_step]\n",
    "        previous_portfolio_value = self.portfolio_value\n",
    "        self.portfolio_value = self.balance + (self.holdings * current_price)\n",
    "        reward = self.portfolio_value - previous_portfolio_value\n",
    "        return reward\n",
    "\n",
    "    def _execute_trade(self, action: int) -> None:\n",
    "        \"\"\"\n",
    "        Executes a trade (buy or sell) and updates balance/holdings.\n",
    "\n",
    "        Args:\n",
    "            action (int): The action taken (1 for buy, 2 for sell).\n",
    "        \"\"\"\n",
    "\n",
    "        current_price = self.data[\"close\"][self.current_step]\n",
    "\n",
    "        if action == 1:  # Buy\n",
    "            affordable_amount = self.balance / current_price\n",
    "            cost = affordable_amount * current_price\n",
    "            fee = cost * self.transaction_fee_percent\n",
    "            if self.balance >= cost + fee:\n",
    "                self.balance -= cost + fee\n",
    "                self.holdings += affordable_amount * (1-self.transaction_fee_percent)\n",
    "\n",
    "        elif action == 2:  # Sell\n",
    "            fee = self.holdings * current_price * self.transaction_fee_percent\n",
    "            self.balance += self.holdings * current_price - fee\n",
    "            self.holdings = 0.0\n",
    "\n",
    "    def step(\n",
    "        self, action: int\n",
    "    ) -> Tuple[np.ndarray, float, bool, bool, dict]:\n",
    "        \"\"\"\n",
    "        Takes a step in the environment.\n",
    "\n",
    "        Args:\n",
    "            action (int): The action to take (0, 1, or 2).\n",
    "\n",
    "        Returns:\n",
    "            Tuple: A tuple containing:\n",
    "                - observation (np.ndarray): The next state's observation.\n",
    "                - reward (float): The reward for the action.\n",
    "                - terminated (bool): Whether the episode has ended.\n",
    "                - truncated (bool): Whether the episode was truncated (not used here, but required by gym API).\n",
    "                - info (dict): Additional information (empty in this case).\n",
    "        \"\"\"\n",
    "        if self._done:\n",
    "            # If the episode is already done, you can either reset it or raise an error.\n",
    "            # Here we reset.  Alternatively, you can raise:  raise Exception(\"Episode already done\")\n",
    "            return self.reset()[0], 0.0, True, False, {}\n",
    "        \n",
    "        self._execute_trade(action)  # Execute the trade first\n",
    "        reward = self._calculate_reward(action)  # Calculate reward *after* trade\n",
    "\n",
    "        self.current_step += 1\n",
    "        # Check if the episode is done (either end of data or out of money)\n",
    "        self._done = (\n",
    "            self.current_step >= len(self.data) - 1\n",
    "            or self.balance <= 0\n",
    "        )\n",
    "        truncated = False\n",
    "        observation = self._get_observation()\n",
    "        info: dict = {}\n",
    "        return observation, reward, self._done, truncated, info  # type: ignore\n",
    "\n",
    "    def reset(\n",
    "        self, *, seed: Optional[int] = None, options: Optional[dict] = None\n",
    "    ) -> Tuple[np.ndarray, dict]:\n",
    "        \"\"\"\n",
    "        Resets the environment to the initial state.\n",
    "\n",
    "        Args:\n",
    "            seed (Optional[int]): Seed for random number generation.\n",
    "            options (Optional[dict]):  Additional options (not used here).\n",
    "\n",
    "        Returns:\n",
    "            Tuple: A tuple containing:\n",
    "                - observation (np.ndarray): The initial observation.\n",
    "                - info (dict):  Additional info (empty in this case).\n",
    "        \"\"\"\n",
    "        super().reset(seed=seed)  # Call gym.Env's reset with the seed\n",
    "\n",
    "        self.current_step = self.lookback_window_size\n",
    "        self.balance = self.initial_balance\n",
    "        self.holdings = 0.0\n",
    "        self.portfolio_value = self.initial_balance\n",
    "        self._done = False  # Reset the done flag\n",
    "        observation = self._get_observation()\n",
    "        info: dict = {}\n",
    "        return observation, info\n",
    "\n",
    "    def render(self) -> None:\n",
    "        \"\"\"\n",
    "        Renders the environment (not implemented here, but could be used for visualization).\n",
    "        \"\"\"\n",
    "        # Future implementation for visualization (e.g., using matplotlib).\n",
    "        if self.render_mode == \"human\":\n",
    "            print(\n",
    "                f\"Step: {self.current_step}, Balance: {self.balance:.2f}, \"\n",
    "                f\"Holdings: {self.holdings:.4f}, Portfolio Value: {self.portfolio_value:.2f}\"\n",
    "            )\n",
    "\n",
    "\n",
    "# Example Usage (and testing)\n",
    "if __name__ == \"__main__\":\n",
    "    # Create sample data (replace with your actual data)\n",
    "    num_days = 200\n",
    "    data = pd.DataFrame(\n",
    "        {\n",
    "            \"close\": np.random.rand(num_days) * 100 + 100,  # Example prices\n",
    "            \"volume\": np.random.rand(num_days) * 1000 + 500,  # Example volume\n",
    "        }\n",
    "    )\n",
    "    data[\"returns\"] = data[\"close\"].pct_change()\n",
    "\n",
    "    # Create and use the environment\n",
    "    env = CryptoTradingEnv(\n",
    "        data=data,\n",
    "        initial_balance=1000,\n",
    "        transaction_fee_percent=0.001,\n",
    "        lookback_window_size=20,\n",
    "        features=[\"close\", \"volume\", \"returns\"],\n",
    "    )\n",
    "\n",
    "    # Basic interaction loop\n",
    "    obs, _ = env.reset()\n",
    "    n_steps = 50\n",
    "    for _ in range(n_steps):\n",
    "        action = env.action_space.sample()  # Replace with your agent's action\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        env.render()\n",
    "        if done:\n",
    "            print(\"Episode finished\")\n",
    "            obs, _ = env.reset()  # Reset if the episode is finished\n",
    "    print(\"Interaction loop finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
